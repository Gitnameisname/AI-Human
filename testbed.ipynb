{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming 응답 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! OpenAI는 인공지능 연구를 위한 비영리 재단입니다. 2015년에 설립되었으며, 목표는 안전하고 유용한 인공지능을 개발하고, 인공지능의 이점을 널리 퍼뜨리는 것입니다. OpenAI는 다양한 인공지능 기술에 대해 연구하고 있으며, 그 결과물로는 ChatGPT와 같은 대화형 AI 모델이 있습니다.</s> "
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"EMPTY\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistral-nemo-12b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Always answer in Korean.\"},\n",
    "        {\"role\": \"user\", \"content\": \"안녕? OpenAI에 대해 알려줄래?\"},\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오픈AI는 인공지능과 관련한 연구를 진행하는 미국의 기업입니다. 2015년에 일론 머스크와 샘 알트만 등 몇 명의 창립자가 설립한 이 회사는 인공지능 기술을 통하여 전 세계의 가치 있는 문제를 해결하고자 노력하고 있습니다. 오픈AI는 다양한 프로젝트와 연구를 진행하고 있으며, 그중 몇 가지는 다음과 같습니다.\n",
      "\n",
      "1. 자연어 처리(NLP) - 오픈AI는 자연어 처리 기술에 중점을 두고 다양한 프로젝트를 진행하고 있습니다. 예를 들어, 대화형 인공지능 어시스턴트인 메타를 개발하였습니다.\n",
      "2. 로보틱스 - 오픈AI는 로보틱스 기술에도 관심을 갖고 있으며, 특히 유인 조종 없이 이동하고 작업을 수행하는 로봇을 개발하고 있습니다.\n",
      "3. 컴퓨터 바둑 - 오픈AI는 인공지능을 바둑에 응용하여, 알파고라는 인공지능 바둑 프로그램을 개발하여 세계 챔피언을 이겼습니다.\n",
      "4. 자율 주행 차량 - 오픈AI는 자율 주행 차량 기술을 연구하고 있으며, 카네기멜론 대학교와 함께 자율 주행 자동차인 카를 개발하였습니다.\n",
      "\n",
      "오픈AI는 인공지능의 안전성과 윤리적인 활용에 대해서도 중요하게 여기며, 이러한 이슈에 관해 다양한 논의와 제도를 제시하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "      model=\"Mistral-Nemo-12b\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Always answer in Korean.\"},\n",
    "        {\"role\": \"user\", \"content\": \"OpenAI에 대해 알려줄래?\"}\n",
    "        ],\n",
    "      stream=False\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/v1/chat/completions\"\n",
    "payload = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, stream=True)\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    if line.detail:\n",
    "        print(line.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import argparse\n",
    "import torch\n",
    "from threading import Thread\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH=\"/Users/ckchoi/Desktop/chatbot/Mistral-Nemo-12b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"안녕?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(messages):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "                        messages,\n",
    "                        return_tensors=\"pt\",\n",
    "                        return_dict=True,\n",
    "                        add_generation_prompt=True,\n",
    "                        truncation=True,\n",
    "                        max_length=1024).to('mps')\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n",
    "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=1024)\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    response = ''\n",
    "    print(f\"streamer: {streamer}\")\n",
    "    for text in streamer:\n",
    "        response += text + ' '\n",
    "        print(f\"text: {text}\")\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in generate_response(messages):\n",
    "    print(f\"chunk: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "                        messages,\n",
    "                        return_tensors=\"pt\",\n",
    "                        return_dict=True,\n",
    "                        add_generation_prompt=True,\n",
    "                        truncation=True,\n",
    "                        max_length=1024).to('mps')\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n",
    "generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=1024)\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "for text in streamer:\n",
    "    print(type(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
